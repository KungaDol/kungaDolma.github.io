[
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "readme.md",
    "section": "",
    "text": "Readme"
  },
  {
    "objectID": "K-Means-Mini-Demo.html",
    "href": "K-Means-Mini-Demo.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\n# YOUR CODE HERE\npenguins &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm) & !is.na(species))\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe’ll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n    \n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer. For reproducibility every time, we run the function \\(kmeans\\), it initializes the centers of the clusters to be in random location. It’s possible that some random location have netter clustering results than others, since \\(kmenas\\) is greedy algorithm, it’s possible for it to have different results each time and its possible for it to get “stuck”at a local soultion.\n\n\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. I think it did pretty good, it found roughly the data points corresponding to each of the specific groups, so it is identifying that as a ways to structure the data.\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. YOUR ANSWER HERE\nLet’s consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\n\n\n\n\n\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\n\n\n\n\n\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\n\n\n\n\n\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. The optimal value for %k% means appear to be k = 3.\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the “K” is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\n\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  },
  {
    "objectID": "Hypothesis/hypothesis.html",
    "href": "Hypothesis/hypothesis.html",
    "title": "hypothesis",
    "section": "",
    "text": "#What is the probability that you would\"reject\"H0 if H0 istrue?\n\nrejcet_ho&lt;- pbinom( q = 6,size = 10, prob = 0.5, lower.tail = FALSE)\n\nfail_to_ho&lt;- pbinom( q = 6,size = 10, prob = 0.7)"
  },
  {
    "objectID": "e_commerce_wrangle.html",
    "href": "e_commerce_wrangle.html",
    "title": "E-commerce",
    "section": "",
    "text": "2. Describe your data set in 1 - 2 sentences.\nThe data set that I choose is the transaction from uk-based online retail store. It has transaction from 01/12/2010 and 09/12/2011. The company mainly sells unique al- occasion gifts.\n\n\n3. What is the source of your data set? Include a link to where you got it here.\nhttps://www.kaggle.com/datasets/carrie1/ecommerce-data\nNote. If you would like to collect survey data from classmates or peers via a Google form, please let me know right away so we can set this up for you. In the meantime, create the Google Form and decide what information you want to collect.\nThe following questions allow us to determine if your proposed data set meets the project criterion:\n\n\n4. Load your data set using the code chunk below.\n\nE_commerce_data &lt;-  read_csv(\"dataset/data.csv\")\n\nRows: 541909 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): InvoiceNo, StockCode, Description, InvoiceDate, Country\ndbl (3): Quantity, UnitPrice, CustomerID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(E_commerce_data)\n\n# A tibble: 6 × 8\n  InvoiceNo StockCode Description      Quantity InvoiceDate UnitPrice CustomerID\n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n1 536365    85123A    WHITE HANGING H…        6 12/1/2010 …      2.55      17850\n2 536365    71053     WHITE METAL LAN…        6 12/1/2010 …      3.39      17850\n3 536365    84406B    CREAM CUPID HEA…        8 12/1/2010 …      2.75      17850\n4 536365    84029G    KNITTED UNION F…        6 12/1/2010 …      3.39      17850\n5 536365    84029E    RED WOOLLY HOTT…        6 12/1/2010 …      3.39      17850\n6 536365    22752     SET 7 BABUSHKA …        2 12/1/2010 …      7.65      17850\n# ℹ 1 more variable: Country &lt;chr&gt;\n\n\n\n\n5. How many rows does your data set have? There should be at least 30 rows.\nThis data set has 541909 rows.\n\n\n6. How many quantitative variables are in your data set? There should be at least 3. List them here and briefly describe what they represent and their units.\n\nQuantity: The number of unit of each product purchased. Unit: Count (integer)\nUnitPrice: The price of a single product. Unit: British Pounds Sterling (GBP).\nTotalPrice( we can create this columns), Total value of each transcation(Quanity * UnitPrice) Unit: British Pounds Sterling (GBP)\n\n#CustomerID: A unique identifier for each customer. #InvoiceNo A unique identifier for each invoice (transaction).\n\n\n7. How many categorical variables does your data set have? There should be at least 1. List the categorical variable(s) here along with their corresponding categories.\n\nDescription: The name or description of the product. Categories: WHITE METAL LANTERN,STAR T-LIGHT HOLDER etc\nCountry: The country where the customer resides. Categories: France, United Kingdom\nstock code: code of each product (item) Categories: 16258A, 90185C etc..\n\nPart 1: Data Context\n\nWhat variables in your data set are you interested in? Are they quantitative or categorical? If quantitative, provide the units. If categorical, list or describe the categories.\n\nQquantitative: 1. Quantity: The number of unit of each product purchased. Unit: Count (integer) 2. UnitPrice: The price of a single product. Unit: British Pounds Sterling (GBP).\ncategorical: 1. Description: The name or description of the product. Categories: WHITE METAL LANTERN,STAR T-LIGHT HOLDER etc 2. Country: The country where the customer resides. Categories: France, United Kingdom, Austrila 3. stock code: code of each product (item) Categories: 16258A, 90185C etc.\n\nWhat does one observational unit (row) represent in your data set? Row unit in the data set represent a single items in a transaction.\nHow was the sample obtained? If you can find what the sampling method was, include that information. Also include what year the data was collected If any of this information isn’t available, state that. How did you access the data? (Include a hyperlink, and the date + time you most recently accessed the data.)\n\nThis data set is publish by Carrie, 8 years ago. The data is collected from 01/12/2010 and 09/12/2011. https://www.kaggle.com/datasets/carrie1/ecommerce-data, I access this data on April 30, 2025 at 1:30pm.\nPart 2: Data Cleaning\n\nDo the variables you listed in (1) have names that are easy to use in code? (Examples of names that might not be easy to work with are those with spaces, those that are very long, etc.). If not, use mutate() and select() to rename your variables (or the rename() function).\n\n\nClean_E_commerce_data &lt;- E_commerce_data |&gt;\n  select(InvoiceNo, StockCode, Description, Quantity, \n         InvoiceDate, InvoiceDate, UnitPrice, CustomerID, Country)|&gt;\n  na.omit()|&gt;\n  rename(\n    \"invoice_no\" = InvoiceNo,\n    \"stock_code\" = StockCode,\n    \"description\" = Description,\n    \"quantity\" = Quantity,\n    \"invoice_date\" = InvoiceDate,\n    \"unit_price\" = UnitPrice,\n    \"customer_id\" = CustomerID,\n    \"country\" = Country\n  )\nhead(Clean_E_commerce_data)\n\n# A tibble: 6 × 8\n  invoice_no stock_code description quantity invoice_date unit_price customer_id\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 536365     85123A     WHITE HANG…        6 12/1/2010 8…       2.55       17850\n2 536365     71053      WHITE META…        6 12/1/2010 8…       3.39       17850\n3 536365     84406B     CREAM CUPI…        8 12/1/2010 8…       2.75       17850\n4 536365     84029G     KNITTED UN…        6 12/1/2010 8…       3.39       17850\n5 536365     84029E     RED WOOLLY…        6 12/1/2010 8…       3.39       17850\n6 536365     22752      SET 7 BABU…        2 12/1/2010 8…       7.65       17850\n# ℹ 1 more variable: country &lt;chr&gt;\n\n\n\nDo your quantitative variables have the correct types (e.g., double for decimals, integer for whole numbers, etc.). If not, set them to be the correct type. You can check the type of a variable using the class() function.\n\n\nglimpse(Clean_E_commerce_data)\n\nRows: 406,829\nColumns: 8\n$ invoice_no   &lt;chr&gt; \"536365\", \"536365\", \"536365\", \"536365\", \"536365\", \"536365…\n$ stock_code   &lt;chr&gt; \"85123A\", \"71053\", \"84406B\", \"84029G\", \"84029E\", \"22752\",…\n$ description  &lt;chr&gt; \"WHITE HANGING HEART T-LIGHT HOLDER\", \"WHITE METAL LANTER…\n$ quantity     &lt;dbl&gt; 6, 6, 8, 6, 6, 2, 6, 6, 6, 32, 6, 6, 8, 6, 6, 3, 2, 3, 3,…\n$ invoice_date &lt;chr&gt; \"12/1/2010 8:26\", \"12/1/2010 8:26\", \"12/1/2010 8:26\", \"12…\n$ unit_price   &lt;dbl&gt; 2.55, 3.39, 2.75, 3.39, 3.39, 7.65, 4.25, 1.85, 1.85, 1.6…\n$ customer_id  &lt;dbl&gt; 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17850, 1…\n$ country      &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Un…\n\n\n\n#changing the invoice_date into date format from character \nClean_E_commerce_data$invoice_date&lt;-as.POSIXct(Clean_E_commerce_data$invoice_date, format = \"%m/%d/%Y %H:%M\")\nClean_E_commerce_data$invoice_date &lt;- as.Date(Clean_E_commerce_data$invoice_date)\n\n\n# convert invoice_no charcater to double \nClean_E_commerce_data &lt;- Clean_E_commerce_data[grepl(\"^[0-9]+$\", Clean_E_commerce_data$invoice_no), ]\nClean_E_commerce_data$invoice_no &lt;- as.double(Clean_E_commerce_data$invoice_no)\n# cahnging description from upper case to lower case. \nClean_E_commerce_data$description &lt;- tolower(Clean_E_commerce_data$description)\n\n\nClean_E_commerce_data$customer_id &lt;- as.integer(Clean_E_commerce_data$customer_id)\n\n\nglimpse(Clean_E_commerce_data)\n\nRows: 397,924\nColumns: 8\n$ invoice_no   &lt;dbl&gt; 536365, 536365, 536365, 536365, 536365, 536365, 536365, 5…\n$ stock_code   &lt;chr&gt; \"85123A\", \"71053\", \"84406B\", \"84029G\", \"84029E\", \"22752\",…\n$ description  &lt;chr&gt; \"white hanging heart t-light holder\", \"white metal lanter…\n$ quantity     &lt;dbl&gt; 6, 6, 8, 6, 6, 2, 6, 6, 6, 32, 6, 6, 8, 6, 6, 3, 2, 3, 3,…\n$ invoice_date &lt;date&gt; 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-…\n$ unit_price   &lt;dbl&gt; 2.55, 3.39, 2.75, 3.39, 3.39, 7.65, 4.25, 1.85, 1.85, 1.6…\n$ customer_id  &lt;int&gt; 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17850, 1…\n$ country      &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Un…\n\n\n\nDo your categorical variables have the type factor? If not, give them this type to tell R that they are categorical.\n\n\nClean_E_commerce_data$country &lt;- factor(Clean_E_commerce_data$country)\nClean_E_commerce_data$description &lt;- factor(Clean_E_commerce_data$description)\n\n\nAre there any variables you need to create? (For example, you might want a variable that combines existing categories, a categorical version of a quantitative variable, etc.)\n\n\nClean_E_commerce_data&lt;- Clean_E_commerce_data|&gt;\n  select(invoice_no, description, stock_code,quantity, invoice_date, unit_price, customer_id, country)|&gt;\n  mutate(total_price = quantity*unit_price)\n\n\nAre there any missing values for your variables listed in (9)? If so, filter them out. Note: Some data sets use empty strings “” to denote missing values, so check for these as well.\n\n\nClean_E_commerce_data &lt;- Clean_E_commerce_data %&gt;%\n  filter(!is.na(invoice_no) & !is.na(description) & !is.na(stock_code)\n         & !is.na(quantity) & !is.na(invoice_date) & !is.na(unit_price)\n         & !is.na(customer_id & !is.na(country)))\n\n\nHow many observational units (rows) do you have after the previous step?\n\n\nnrow(Clean_E_commerce_data)\n\n[1] 397924\n\n\n\nDoes the source of data say that certain data points are missing? (For example, in NHANES we are told that the variable Height is measured only for participants aged 2 years or older, meaning Height will have NA values in rows corresponding to participants under 2.) If not available in the data description, why do you think some data points are missing? Is the mechanism for missingness completely random, or could there be something systematic which leads to greater rates of missingness? It’s okay if you’re not sure how to answer this question — just do your best to think about it.\n\nseem like this data set doesn’t contain any missing points. even if there were some missing point, it might be systematic not mechanism.\n\ncount(Clean_E_commerce_data$country)\n\nn_Australia \n       1185 \n\n\n\nClean_E_commerce_data %&gt;% count(invoice_date)\n\n# A tibble: 306 × 2\n   invoice_date     n\n   &lt;date&gt;       &lt;int&gt;\n 1 2010-12-01    1942\n 2 2010-12-02    1895\n 3 2010-12-03    1208\n 4 2010-12-05    2708\n 5 2010-12-06    1945\n 6 2010-12-07    1110\n 7 2010-12-08    1948\n 8 2010-12-09    1616\n 9 2010-12-10    1513\n10 2010-12-12    1436\n# ℹ 296 more rows\n\n\nThe dataset is large to analyze year from 01/12/2010 and 09/12/2011 because of this it hard to see the visualiztion properly. I decided that I will only focused on 1 month .\n\ne_commerce_2011 &lt;- Clean_E_commerce_data|&gt;\n  filter(invoice_date &gt;= as.Date(\"2011-07-01\") & invoice_date &lt; as.Date(\"2011-08-01\"))\n\n\ne_commerce_2011&lt;- e_commerce_2011 %&gt;% \n  filter(country %in% c( \"Germany\",\"EIRE\", \"France\", \"Spain\",\n                         \"Australia\", \"Belgium\", \"Switzerland\",\n                        \"Portugal\",\"Finland\", \"Canada\"))\n\nPart 3: Exploratory Data Analysis Provide any numerical summaries that are relevant to your research question. These are highly dependent on your question, but some ideas are:\nFor one quantitative variable, some relevant numerical summaries involve: One measure of center (e.g., mean, median); One measure of spread (e.g., standard deviation, variance, IQR, a minimum/maximum); An overall count (how many observations do you have?)\n\nsummary(e_commerce_2011)\n\n   invoice_no                                 description    stock_code       \n Min.   :558639   postage                           :  59   Length:2662       \n 1st Qu.:559557   regency cakestand 3 tier          :  22   Class :character  \n Median :560398   round snack boxes set of4 woodland:  17   Mode  :character  \n Mean   :560291   lunch bag red retrospot           :  16                     \n 3rd Qu.:560997   plasters in tin woodland animals  :  16                     \n Max.   :561901   set of 3 regency cake tins        :  16                     \n                  (Other)                           :2516                     \n    quantity       invoice_date          unit_price       customer_id   \n Min.   :  1.00   Min.   :2011-07-01   Min.   :  0.000   Min.   :12362  \n 1st Qu.:  6.00   1st Qu.:2011-07-11   1st Qu.:  0.850   1st Qu.:12490  \n Median : 12.00   Median :2011-07-18   Median :  1.650   Median :12662  \n Mean   : 19.46   Mean   :2011-07-16   Mean   :  3.713   Mean   :13244  \n 3rd Qu.: 16.00   3rd Qu.:2011-07-22   3rd Qu.:  3.750   3rd Qu.:14016  \n Max.   :432.00   Max.   :2011-07-31   Max.   :550.940   Max.   :17444  \n                                                                        \n      country     total_price     \n Germany  :755   Min.   :   0.00  \n EIRE     :616   1st Qu.:  12.48  \n France   :448   Median :  16.60  \n Spain    :180   Mean   :  35.02  \n Australia:159   3rd Qu.:  24.96  \n Belgium  :128   Max.   :2365.20  \n (Other)  :376                    \n\ncount(e_commerce_2011)\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  2662\n\n\n\nsummary(e_commerce_2011$unit_price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.850   1.650   3.713   3.750 550.940 \n\n\n\nsd(e_commerce_2011$unit_price)\n\n[1] 14.16904\n\nvar(e_commerce_2011$unit_price)\n\n[1] 200.7616\n\nIQR(e_commerce_2011$unit_price)\n\n[1] 2.9\n\n\nFor one categorical variable, a relevant numerical summary might be: the # of observations in each category (i.e., use count()); the proportion (or percentage) of observations in each category.\n\ne_commerce_2011|&gt;\n  count(country)|&gt;\n  mutate(\n    proportion = n/sum(n),\n    percentage = round((n / sum(n)) * 100, 2)\n  )\n\n# A tibble: 10 × 4\n   country         n proportion percentage\n   &lt;fct&gt;       &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Australia     159     0.0597       5.97\n 2 Belgium       128     0.0481       4.81\n 3 Canada         78     0.0293       2.93\n 4 EIRE          616     0.231       23.1 \n 5 Finland        83     0.0312       3.12\n 6 France        448     0.168       16.8 \n 7 Germany       755     0.284       28.4 \n 8 Portugal       89     0.0334       3.34\n 9 Spain         180     0.0676       6.76\n10 Switzerland   126     0.0473       4.73\n\n\n\ne_commerce_2011|&gt;\n  count(stock_code)|&gt;\n  mutate(\n    proportion = n/sum(n),\n    percentage = round((n / sum(n)) * 100, 2)\n  )\n\n# A tibble: 1,014 × 4\n   stock_code     n proportion percentage\n   &lt;chr&gt;      &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 10125          1   0.000376       0.04\n 2 10133          2   0.000751       0.08\n 3 11001          2   0.000751       0.08\n 4 15036          1   0.000376       0.04\n 5 15044A         1   0.000376       0.04\n 6 15044C         1   0.000376       0.04\n 7 15044D         1   0.000376       0.04\n 8 15056BL        4   0.00150        0.15\n 9 15056N         2   0.000751       0.08\n10 15056P         2   0.000751       0.08\n# ℹ 1,004 more rows\n\n\n\ne_commerce_2011|&gt;\n  count(description)|&gt;\n  mutate(\n    proportion = n/sum(n),\n    percentage = round((n / sum(n)) * 100, 2)\n  )\n\n# A tibble: 1,014 × 4\n   description                            n proportion percentage\n   &lt;fct&gt;                              &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 10 colour spaceboy pen                 4   0.00150        0.15\n 2 12 coloured party balloons             2   0.000751       0.08\n 3 12 message cards with envelopes        1   0.000376       0.04\n 4 12 pencils tall tube posy              3   0.00113        0.11\n 5 12 pencils tall tube red retrospot     1   0.000376       0.04\n 6 12 pencils tall tube woodland          3   0.00113        0.11\n 7 18pc wooden cutlery set disposable     3   0.00113        0.11\n 8 20 dolly pegs retrospot                1   0.000376       0.04\n 9 200 red + white bendy straws           1   0.000376       0.04\n10 3 hook hanger magic garden             2   0.000751       0.08\n# ℹ 1,004 more rows\n\n\nProvide any visualizations that are relevant to your research question. Chapter 2 in this online text is a good resource for creating different plots.\nThe choice of visualization can be highly dependent on your question, but some ideas are: 1. A visual summary of your (quantitative) outcome of interest: Examples: a density plot, a box plot, a histogram, a relative frequency histogram, etc.\n\ne_commerce_2011 |&gt;\n  ggplot( aes(x= unit_price)) +\n  geom_density(fill=\"#69b3a2\", color=\"#e9ecef\") + xlim(0,50)\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nA visual summary of your predictor(s) of interest: For one quantitative predictor, the examples in the previous bullet point apply For one categorical predictor, you could do a bar chart, mosaic plot, etc.\n\n\nlibrary(forcats)\nggplot(e_commerce_2011, aes(x= fct_infreq(country), y=unit_price)) + \n  geom_bar(stat = \"identity\") + \n   theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\n\nA visual summary of the relationship between one quantitative variable and one or more different variables: Ideas: scatter plots (maybe with points colored by a category of some categorical variable), side-by-side histograms, side-by-side box plots, side-by-side histograms, overlaid density plots, etc.\n\n\nremoved_three_coulmns&lt;-e_commerce_2011|&gt;\n  select(invoice_no,quantity, invoice_date, unit_price, customer_id)\n\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nggpairs(removed_three_coulmns)\n\n\n\n\n\n\n\n\n\nsave(e_commerce_2011, file = \"dataset/e_commerce_2011.Rdata\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my website! I’m a senior at Mount Holyoke College, majoring in Statistics. I’m originally from Nepal and Tibet, and I celebrate Tibetan Losar, one of my favorite cultural traditions.\nI enjoy reading Webtoons, watching football, and hiking in my free time. My academic and professional interests include sports analytics, marketing, and health data analysis.\nFeel free to Connect with me on LinkedIn"
  },
  {
    "objectID": "Hypothesis/HW8 (1).html",
    "href": "Hypothesis/HW8 (1).html",
    "title": "Homework 8",
    "section": "",
    "text": "This assignment is due at 11:59 PM on Thursday, May 1st.\n\n\n\nAll problems will be graded for correctness. In grading these problems, an emphasis will be placed on full explanations of your thought process. You don’t need to write more than a few sentences for any given problem, but you should write complete sentences! Understanding and explaining the reasons behind what you are doing is at least as important as solving the problems correctly.\n\n\n\nYou are allowed to work with others on this assignment, but you must complete and submit your own write up. You should not copy large blocks of code or written text from another student.\n\n\n\nAll sources you refer to must be cited at the end of the problem set.\n\n\n\n\nFor each of these statements, state whether they are true or false, and why:\n\n\nFalse, because the significance level of a statistical test is the probability making type I error given null hypothesis is true.\n\n\n\nFalse, the significance level of a test is decreased, then it will be hard to reject null hypothesis decreases. Since the power is the 1- Beta (probability of type error II when alternative hypothesis is true)then the power should decrease as well.\n\n\n\nTrue, because in the case of covid testing, a Type II error means telling a person they don’t have COVID when they actually do. This wrong information can be dangerous because they might not take the precaution or treatment to cure the disease.\nWhile in Type I error case it means saying someone has covid when they don’t. This information, won’t have an negative effect on their health.\n\n\n\nFalse, the power of a test under alternate hypothesis. It is the probability of rejecting the null hypothesis when the alternate hypothesis is true.\n\n\n\n\nLet \\(X\\) have one of the following distributions:\nWe want to conduct the test:\n\\(H_0\\): the first distribution is correct \\(H_A\\): the second distribution is correct\n\n\n\nX = c(\"x1\", \"x2\",\"x3\", \"x4\")\nh_0 = c(0.2,0.3,0.3,0.2)\nh_a = c(0.1,0.4,0.1,0.4)\nlikelihood_ratio = h_0/h_a\nlikelihood_ratio \n\n[1] 2.00 0.75 3.00 0.50\n\ndistribution&lt;- data.frame(X = c(\"x1\", \"x2\",\"x3\", \"x4\"),\n                          h_0 = c(0.2,0.3,0.3,0.2),\n                          h_a = c(0.1,0.4,0.1,0.4),\n                          likelihood_ratio = likelihood_ratio)\n\n\ndistribution %&gt;% \n  arrange(desc(likelihood_ratio))\n\n   X h_0 h_a likelihood_ratio\n1 x3 0.3 0.1             3.00\n2 x1 0.2 0.1             2.00\n3 x2 0.3 0.4             0.75\n4 x4 0.2 0.4             0.50\n\n\n\n\n\nPr(\\(\\Lambda(X)\\) = \\(\\Lambda(xi)\\) | H_0 is true)\nPr(\\(\\Lambda(X)\\) = 2.00 | H_0 is true) = 0.2\nPr(\\(\\Lambda(X)\\) = 0.75| H_0 is true) = 0.3\nPr(\\(\\Lambda(X)\\) = 3.00 | H_0 is true) = 0.3\nPr(\\(\\Lambda(X)\\) = 0.50 | H_0 is true) = 0.2\n\n\n\nPr(\\(\\Lambda(X)\\) =&lt; \\(\\Lambda(xi)\\) | H_0 is true)\nPr(\\(\\Lambda(X)\\) =&lt; x_3 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 3.0 | H_0 is true) = 0.2+0.3+0.2+0.3 = 1.0\nPr(\\(\\Lambda(X)\\) =&lt; x_4 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 0.50 | H_0 is true) = 0.2\nPr(\\(\\Lambda(X)\\) =&lt; x_2 | H_0 is true) Pr(\\(\\Lambda(X)\\) = 0.75 | H_0 is true) = 0.3 +0.2 = 0.5\nPr(\\(\\Lambda(X)\\) =&lt; x_1 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 2.00 | H_0 is true) = 02 + 0.3 + 0.2 = 0.7\nthe possible p-values for the likelihood ratio test is 0.2, 0.5, 0.7,1.0\n\n\n\n\ncitation:\n\nchapter 9_hypothesis Testing lecture note\nchapter 6"
  },
  {
    "objectID": "Hypothesis/HW8 (1).html#problem-1",
    "href": "Hypothesis/HW8 (1).html#problem-1",
    "title": "Homework 8",
    "section": "",
    "text": "For each of these statements, state whether they are true or false, and why:\n\n\nFalse, because the significance level of a statistical test is the probability making type I error given null hypothesis is true.\n\n\n\nFalse, the significance level of a test is decreased, then it will be hard to reject null hypothesis decreases. Since the power is the 1- Beta (probability of type error II when alternative hypothesis is true)then the power should decrease as well.\n\n\n\nTrue, because in the case of covid testing, a Type II error means telling a person they don’t have COVID when they actually do. This wrong information can be dangerous because they might not take the precaution or treatment to cure the disease.\nWhile in Type I error case it means saying someone has covid when they don’t. This information, won’t have an negative effect on their health.\n\n\n\nFalse, the power of a test under alternate hypothesis. It is the probability of rejecting the null hypothesis when the alternate hypothesis is true."
  },
  {
    "objectID": "Hypothesis/HW8 (1).html#problem-2",
    "href": "Hypothesis/HW8 (1).html#problem-2",
    "title": "Homework 8",
    "section": "",
    "text": "Let \\(X\\) have one of the following distributions:\nWe want to conduct the test:\n\\(H_0\\): the first distribution is correct \\(H_A\\): the second distribution is correct\n\n\n\nX = c(\"x1\", \"x2\",\"x3\", \"x4\")\nh_0 = c(0.2,0.3,0.3,0.2)\nh_a = c(0.1,0.4,0.1,0.4)\nlikelihood_ratio = h_0/h_a\nlikelihood_ratio \n\n[1] 2.00 0.75 3.00 0.50\n\ndistribution&lt;- data.frame(X = c(\"x1\", \"x2\",\"x3\", \"x4\"),\n                          h_0 = c(0.2,0.3,0.3,0.2),\n                          h_a = c(0.1,0.4,0.1,0.4),\n                          likelihood_ratio = likelihood_ratio)\n\n\ndistribution %&gt;% \n  arrange(desc(likelihood_ratio))\n\n   X h_0 h_a likelihood_ratio\n1 x3 0.3 0.1             3.00\n2 x1 0.2 0.1             2.00\n3 x2 0.3 0.4             0.75\n4 x4 0.2 0.4             0.50\n\n\n\n\n\nPr(\\(\\Lambda(X)\\) = \\(\\Lambda(xi)\\) | H_0 is true)\nPr(\\(\\Lambda(X)\\) = 2.00 | H_0 is true) = 0.2\nPr(\\(\\Lambda(X)\\) = 0.75| H_0 is true) = 0.3\nPr(\\(\\Lambda(X)\\) = 3.00 | H_0 is true) = 0.3\nPr(\\(\\Lambda(X)\\) = 0.50 | H_0 is true) = 0.2\n\n\n\nPr(\\(\\Lambda(X)\\) =&lt; \\(\\Lambda(xi)\\) | H_0 is true)\nPr(\\(\\Lambda(X)\\) =&lt; x_3 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 3.0 | H_0 is true) = 0.2+0.3+0.2+0.3 = 1.0\nPr(\\(\\Lambda(X)\\) =&lt; x_4 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 0.50 | H_0 is true) = 0.2\nPr(\\(\\Lambda(X)\\) =&lt; x_2 | H_0 is true) Pr(\\(\\Lambda(X)\\) = 0.75 | H_0 is true) = 0.3 +0.2 = 0.5\nPr(\\(\\Lambda(X)\\) =&lt; x_1 | H_0 is true) Pr(\\(\\Lambda(X)\\) =&lt; 2.00 | H_0 is true) = 02 + 0.3 + 0.2 = 0.7\nthe possible p-values for the likelihood ratio test is 0.2, 0.5, 0.7,1.0"
  },
  {
    "objectID": "Hypothesis/HW8 (1).html#problem-3",
    "href": "Hypothesis/HW8 (1).html#problem-3",
    "title": "Homework 8",
    "section": "",
    "text": "citation:\n\nchapter 9_hypothesis Testing lecture note\nchapter 6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my homepage!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\ncoin_0 &lt;- dbinom(x = 2, size = 10, prob = 0.5)/dbinom(x = 2, size = 10, prob = 0.7)\ncoin_0\n\n[1] 30.37623\n\ncoin_1 &lt;- dbinom(x = 9, size = 10, prob = 0.5)/dbinom(x = 9, size = 10, prob = 0.7)\ncoin_1\n\n[1] 0.0806671\n\n\n0.8066 less likely to obtain 9 head in 10 trail\n\npbinom( q = 6, size = 10, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.171875\n\npbinom( q = 6, size = 10, prob = 0.7, lower.tail = TRUE)\n\n[1] 0.3503893"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "STAT 244-SC",
    "section": "",
    "text": "Introduction\nThe data set that I choose is the transaction from uk-based online retail store. It has transaction from 01/12/2010 and 09/12/2011. The company mainly sells unique all- occasion gifts. This data set has 541909 rows. The quantitative variables that this datset have: 1. Quantity: The number of unit of each product purchased. Unit: Count (integer) 2. UnitPrice: The price of a single product. Unit: British Pounds Sterling (GBP). 3. TotalPrice( we can create this columns), Total value of each transcation(Quanity * UnitPrice) Unit: British Pounds Sterling (GBP)\nThe categorical variables present in this dataset: 1. Description: The name or description of the product. Categories: WHITE METAL LANTERN,STAR T-LIGHT HOLDER etc 2. Country: The country where the customer resides. Categories: France, United Kingdom 3. stock code: code of each product (item) Categories: 16258A, 90185C etc..\nhttps://www.kaggle.com/datasets/carrie1/ecommerce-data\n#load packages\n\nload(\"dataset/e_commerce_2011.Rdata\")\n\n\n\nData visualization\n\ne_commerce_2011 |&gt;\n  ggplot( aes(x= unit_price)) +\n  geom_density(fill=\"#69b3a2\", color=\"#e9ecef\") + xlim(0,50)\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(forcats)\nggplot(e_commerce_2011, aes(x= fct_infreq(country), y=unit_price)) + \n  geom_bar(stat = \"identity\") + \n   theme(axis.text.x = element_text(angle = 90))"
  }
]